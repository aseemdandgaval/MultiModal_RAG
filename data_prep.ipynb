{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAQ5jv8SyNB_"
      },
      "source": [
        "# **1. Importing Libraies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "10sf7csfz15p"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Install Dependencies (Linux Only)\n",
        "!pip install -U langchain openai langchain-chroma langchain-experimental langchain_openai\n",
        "!pip install \"unstructured[pdf]\" pillow pydantic lxml pillow matplotlib tiktoken open_clip_torch\n",
        "!apt-get install poppler-utils tesseract-ocr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaTxY_Vj8RcK",
        "outputId": "bf6420ce-086b-45c3-cf61-48502233804b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import uuid\n",
        "import nltk\n",
        "import base64\n",
        "import chromadb\n",
        "import numpy as np\n",
        "from PIL import Image as _PILImage\n",
        "from IPython.display import HTML, display\n",
        "from unstructured.partition.pdf import partition_pdf\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain.schema.document import Document\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vCDtUbDUW-my"
      },
      "outputs": [],
      "source": [
        "# Set API keys\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
        "os.environ[\"GROQ_API_KEY\"] = \"gsk..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0dMyuH8zNKx"
      },
      "source": [
        "# **2. Extracting Text, Images and Tables from the PDF**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "H0cVNG8N8Mj6"
      },
      "outputs": [],
      "source": [
        "# Specify pdf path and folder to store images\n",
        "file_path = \"paper.pdf\"\n",
        "output_path = \"images/\"\n",
        "\n",
        "if not os.path.exists(output_path):\n",
        "    os.makedirs(output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zN5JuxB8z6yB"
      },
      "outputs": [],
      "source": [
        "# Parsing the PDF to extract chunks\n",
        "# Reference: https://docs.unstructured.io/open-source/core-functionality/chunking\n",
        "chunks = partition_pdf(\n",
        "    filename=file_path,\n",
        "    infer_table_structure=True,                 # extract tables\n",
        "    strategy=\"hi_res\",                          # mandatory to infer tables\n",
        "    extract_image_block_types=[\"Image\", \"Table\"],  # Add 'Table' to list to extract image of tables\n",
        "    image_output_dir_path=output_path,             # if None, images and tables will saved in base64\n",
        "    extract_image_block_to_payload=True,   # if true, will extract base64 for API usage\n",
        "    chunking_strategy=\"by_title\",          # or 'basic'\n",
        "    max_characters=1200,                   # defaults to 500\n",
        "    combine_text_under_n_chars= 100,        # defaults to 0\n",
        "    new_after_n_chars=100,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "J_MdKdpVwPps"
      },
      "outputs": [],
      "source": [
        "# Extract Text, Tables and Images from Composite Elements\n",
        "texts = []\n",
        "tables = []\n",
        "tables_html = []\n",
        "images_b64 = []\n",
        "\n",
        "for composite_element in chunks:\n",
        "  for sub_element in composite_element.metadata.orig_elements:\n",
        "    if \"Table\" in str(type(sub_element)):\n",
        "        tables.append(str(sub_element))\n",
        "        tables_html.append(sub_element.metadata.text_as_html)\n",
        "    elif \"Image\" in str(type(sub_element)):\n",
        "        images_b64.append(sub_element.metadata.image_base64)\n",
        "    elif \"Footer\" not in str(type(sub_element)):\n",
        "        texts.append(str(sub_element))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilguB8ymR-CS",
        "outputId": "6291aefa-0fd8-4ba2-d049-9ac98a53d872"
      },
      "outputs": [],
      "source": [
        "# Combine short strings into bigger ones\n",
        "def combine_short_strings(strings, threshold):\n",
        "    result = []\n",
        "    temp = \"\"\n",
        "    for string in strings:\n",
        "        if temp:\n",
        "            temp += \" \" + string\n",
        "        else:\n",
        "            temp = string\n",
        "\n",
        "        if len(temp) >= threshold:\n",
        "            result.append(temp)\n",
        "            temp = \"\"\n",
        "\n",
        "    if temp:\n",
        "        result.append(temp)\n",
        "\n",
        "    return result\n",
        "\n",
        "threshold = 50\n",
        "texts = combine_short_strings(texts, threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "zcRmA5fWSBls"
      },
      "outputs": [],
      "source": [
        "# Save the images to a file\n",
        "for i in range(len(images_b64)):\n",
        "    image_data = base64.b64decode(images_b64[i])\n",
        "    output_file = output_path + f\"image_{i}.jpg\"\n",
        "    with open(output_file, \"wb\") as file:\n",
        "        file.write(image_data)\n",
        "\n",
        "# Get image URIs with .jpg extension only\n",
        "image_paths = sorted(\n",
        "    [\n",
        "        os.path.join(output_path, image_name)\n",
        "        for image_name in os.listdir(output_path)\n",
        "        if image_name.endswith(\".jpg\")\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV9pe-PMUF0D"
      },
      "source": [
        "# **3. Summarizing Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "jrLwoGpEU0n4"
      },
      "outputs": [],
      "source": [
        "# Prompt\n",
        "prompt_table = \"\"\"You are an assistant tasked with summarizing tables and text for retrieval. \\\n",
        "These summaries will be embedded and used to retrieve the raw text or table elements. \\\n",
        "Give a concise summary of the table or text that is well optimized for retrieval in markdown format.\n",
        "Table or text: {element} \"\"\"\n",
        "\n",
        "prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text for retrieval. \\\n",
        "These summaries will be embedded and used to retrieve the raw text or table elements. \\\n",
        "Give a comprehensive summary of the table or text that is well optimized for retrieval in markdown format.\n",
        "Table or text: {element} \"\"\"\n",
        "\n",
        "prompt_text = ChatPromptTemplate.from_template(prompt_text)\n",
        "prompt_table = ChatPromptTemplate.from_template(prompt_table)\n",
        "\n",
        "# Text summary chain\n",
        "model = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
        "summarize_chain_text = {\"element\": lambda x: x} | prompt_text | model | StrOutputParser()\n",
        "summarize_chain_table = {\"element\": lambda x: x} | prompt_table | model | StrOutputParser()\n",
        "\n",
        "# Summarize\n",
        "text_summaries = summarize_chain_text.batch(texts, {\"max_concurrency\": 3})\n",
        "table_summaries = summarize_chain_table.batch(tables_html, {\"max_concurrency\": 3})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "LtQFQruT-JBW"
      },
      "outputs": [],
      "source": [
        "def encode_image(image_path):\n",
        "    \"\"\"Getting the base64 string\"\"\"\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
        "\n",
        "def image_summarize(img_base64, prompt):\n",
        "    \"\"\"Make image summary\"\"\"\n",
        "    chat = ChatOpenAI(model=\"gpt-4o-mini\", max_tokens=1024)\n",
        "\n",
        "    msg = chat.invoke(\n",
        "          [HumanMessage(\n",
        "              content=[\n",
        "                    {\"type\": \"text\", \"text\": prompt},\n",
        "                    {\"type\": \"image_url\",\n",
        "                     \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},},\n",
        "                ]\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "    return msg.content\n",
        "\n",
        "prompt = \"\"\"Describe the image in detail. For context,\n",
        "                  the image is part of a research paper explaining the transformers\n",
        "                  architecture. Be specific about graphs, such as bar plots.\"\"\"\n",
        "\n",
        "image_summaries = []\n",
        "for image in images_b64:\n",
        "    image_summaries.append(image_summarize(image, prompt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "z7qWg2YeFQ_G",
        "outputId": "eee86edf-f39e-489c-af00-f354acaecfd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# Summary of Transformer Sequence Lengths Trend\n",
            "\n",
            "**Figure Title:** Trend of Transformer Sequence Lengths Over Time\n",
            "\n",
            "**Key Points:**\n",
            "- The figure illustrates the trend of sequence lengths used in Transformer models over a specified time period.\n",
            "- The data is represented in millions of dollars, with a focus on values less than 5 million.\n",
            "- The trend indicates fluctuations in sequence lengths, which may correlate with advancements in model architecture or training techniques.\n",
            "\n",
            "**Optimization for Retrieval:**\n",
            "- **Keywords:** Transformer models, sequence lengths, trend analysis, time series, machine learning, model architecture.\n",
            "- **Context:** Useful for understanding the evolution of Transformer models and their efficiency in handling varying sequence lengths.\n",
            "\n",
            "This summary encapsulates the essential information regarding the trend of Transformer sequence lengths, making it easy to retrieve the relevant data or figure.\n"
          ]
        }
      ],
      "source": [
        "print(text_summaries[5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6aEmJFA6FSS-",
        "outputId": "e6efcb02-f4fa-456b-aad7-d6f2c388a9c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### Summary of Computation Complexity Methods\n",
            "\n",
            "| Method                | Computation Complexity |\n",
            "|-----------------------|-----------------------|\n",
            "| Recurrent             | O(Nd?)                |\n",
            "| Vanilla Attention     | O(N? d)               |\n",
            "| Sparse Attention      | O(NVNd)               |\n",
            "| Dilated Attention     | O(Nd)                 |\n",
            "\n",
            "This table summarizes various methods and their corresponding computation complexities.\n"
          ]
        }
      ],
      "source": [
        "print(table_summaries[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "14aRkYb8B1Ff",
        "outputId": "80ed00ac-d32c-43ad-8523-3a19c4708c80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The image presents a line graph illustrating the evolution of transformer architectures over time, spanning from the year 2017 to 2023. The x-axis denotes the years, while the y-axis represents the model sizes in terms of parameters, with values ranging from 0 to 1000.\n",
            "\n",
            "Key features of the graph include:\n",
            "\n",
            "1. **Data Points**: Each point on the graph corresponds to a specific transformer model, annotated with its name and size:\n",
            "   - **GPT (512)**, placed at the year 2018.\n",
            "   - **Sparse Transformer (12K)**, positioned slightly to the right in 2019.\n",
            "   - **Reformer (64K)**, located in 2020.\n",
            "   - **Memorizing Transformers (262K)**, appearing in 2022.\n",
            "   - **RMT (1M)**, marked for the year 2022, indicating a substantial increase in model size.\n",
            "\n",
            "2. **LongNet (1B)**: The most significant point on the graph, labeled in red, is LongNet (1B). This point is prominently at the far right on the 2023 mark, with a value exceeding 1000, signifying a dramatic increase in model size compared to its predecessors.\n",
            "\n",
            "3. **Stylistic Elements**: \n",
            "   - The graph features a dashed red line connecting the data points up to LongNet, emphasizing the progression toward larger transformer models.\n",
            "   - Each data point is indicated by a dot, with size corresponding to the number of parameters, enhancing the visual representation of the growth in model complexity.\n",
            "\n",
            "Overall, the graph effectively illustrates the increasing scale of transformer models over time, highlighting key developments in the field.\n"
          ]
        }
      ],
      "source": [
        "print(image_summaries[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2r3AX9iyzUoD"
      },
      "source": [
        "# **3. Creating a Multimodal Vectorstore (ChromaDB)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ydnrhWhDt-i",
        "outputId": "ff870920-e06b-49e3-c186-b490ad363048"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-42-20c637d79a48>:10: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
            "  embedding_function=OpenAIEmbeddings(),\n",
            "<ipython-input-42-20c637d79a48>:9: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
            "  vectorstore = Chroma(collection_name=\"multi_modal_rag\",\n"
          ]
        }
      ],
      "source": [
        "# The vectorstore to use to index the child chunks\n",
        "vectorstore = Chroma(collection_name=\"multi_modal_rag\",\n",
        "                     embedding_function=OpenAIEmbeddings(),\n",
        "                     persist_directory=\"chroma_langchain_db\")\n",
        "\n",
        "# The storage layer for the parent documents\n",
        "id_key = \"doc_id\"\n",
        "store = InMemoryStore()\n",
        "\n",
        "# The retriever (empty to start)\n",
        "retriever = MultiVectorRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    docstore=store,\n",
        "    id_key=id_key,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "gRhELX81MGuf"
      },
      "outputs": [],
      "source": [
        "# Add texts\n",
        "doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
        "summary_texts = [Document(page_content=summary, metadata={id_key: doc_ids[i]}) for i, summary in enumerate(text_summaries)]\n",
        "retriever.vectorstore.add_documents(summary_texts)\n",
        "retriever.docstore.mset(list(zip(doc_ids, texts)))\n",
        "\n",
        "# Add tables\n",
        "table_ids = [str(uuid.uuid4()) for _ in tables]\n",
        "summary_tables = [Document(page_content=summary, metadata={id_key: table_ids[i]}) for i, summary in enumerate(table_summaries)]\n",
        "retriever.vectorstore.add_documents(summary_tables)\n",
        "retriever.docstore.mset(list(zip(table_ids, tables)))\n",
        "\n",
        "# Add image summaries\n",
        "img_ids = [str(uuid.uuid4()) for _ in images_b64]\n",
        "summary_img = [Document(page_content=summary, metadata={id_key: img_ids[i]}) for i, summary in enumerate(image_summaries)]\n",
        "retriever.vectorstore.add_documents(summary_img)\n",
        "retriever.docstore.mset(list(zip(img_ids, images_b64)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vX9FXHn4CctX"
      },
      "source": [
        "### **Download the vectorstore and images to use locally**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "25SNz8IpCb0j",
        "outputId": "cd8a18e4-39d6-40f8-e15f-492ea51b2fe3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: content/chroma_langchain_db/ (stored 0%)\n",
            "  adding: content/chroma_langchain_db/0029ac48-b5c5-4755-9b42-a61f8e18b33c/ (stored 0%)\n",
            "  adding: content/chroma_langchain_db/0029ac48-b5c5-4755-9b42-a61f8e18b33c/data_level0.bin (deflated 65%)\n",
            "  adding: content/chroma_langchain_db/0029ac48-b5c5-4755-9b42-a61f8e18b33c/length.bin (deflated 51%)\n",
            "  adding: content/chroma_langchain_db/0029ac48-b5c5-4755-9b42-a61f8e18b33c/link_lists.bin (stored 0%)\n",
            "  adding: content/chroma_langchain_db/0029ac48-b5c5-4755-9b42-a61f8e18b33c/header.bin (deflated 61%)\n",
            "  adding: content/chroma_langchain_db/chroma.sqlite3 (deflated 55%)\n",
            "  adding: content/images/ (stored 0%)\n",
            "  adding: content/images/image_1.jpg (deflated 35%)\n",
            "  adding: content/images/image_5.jpg (deflated 29%)\n",
            "  adding: content/images/image_4.jpg (deflated 37%)\n",
            "  adding: content/images/image_7.jpg (deflated 27%)\n",
            "  adding: content/images/image_6.jpg (deflated 33%)\n",
            "  adding: content/images/image_0.jpg (deflated 32%)\n",
            "  adding: content/images/image_3.jpg (deflated 42%)\n",
            "  adding: content/images/image_2.jpg (deflated 23%)\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_fccf9da6-d6c7-41d0-a4a2-ff712d430e17\", \"db.zip\", 3531898)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_5c5f514b-c0e5-41b9-b6d1-a2c2761ead77\", \"images.zip\", 202190)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "!zip -r /content/db.zip /content/chroma_langchain_db/\n",
        "!zip -r /content/images.zip /content/images/\n",
        "\n",
        "files.download(\"/content/db.zip\")\n",
        "files.download(\"/content/images.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuI6001QCTKd"
      },
      "source": [
        "# **4. Retreival Sanity Check**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "e7SArwGR8rFc"
      },
      "outputs": [],
      "source": [
        "def is_base64(s):\n",
        "    \"\"\"Check if a string is Base64 encoded\"\"\"\n",
        "    try:\n",
        "        return base64.b64encode(base64.b64decode(s)) == s.encode()\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def plt_img_base64(img_base64):\n",
        "    # Create an HTML img tag with the base64 string as the source\n",
        "    image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n",
        "\n",
        "    # Display the image by rendering the HTML\n",
        "    display(HTML(image_html))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNfUbwHAEBDN",
        "outputId": "5c228480-fc56-4f27-857a-8de56e06a9a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# Summary of Dilated Attention\n",
            "\n",
            "## Overview\n",
            "Dilated attention is a mechanism that processes input data by dividing it into segments and applying sparsification. \n",
            "\n",
            "## Key Components\n",
            "- **Input Variables**: The input consists of three components: Query (Q), Key (K), and Value (V).\n",
            "- **Segmentation**: The input is split into N segments, each of length w, resulting in segments denoted as {( ̃Qi, ̃Ki, ̃Vi)}.\n",
            "- **Sparsification**: Each segment undergoes a sparsification process along the sequence dimension by selecting rows at a specified interval r.\n",
            "\n",
            "## Computation\n",
            "The computation of dilated attention can be expressed mathematically, although the specific formula is not provided in the text.\n",
            "\n",
            "This summary encapsulates the main aspects of dilated attention, focusing on its structure and processing method, making it suitable for retrieval purposes.\n",
            "\n",
            "---------\n",
            "# Summary of Dilated Attention Mechanism\n",
            "\n",
            "The text discusses the implementation of dilated attention, represented as \\( O = [ \\hat{O}_0, \\hat{O}_1, \\ldots, \\hat{O}_{N_w - 1} ] \\). Key points include:\n",
            "\n",
            "- **Transformation**: Dilated attention can be converted into dense attention through a gathering operation on the input (Query \\( Q \\), Key \\( K \\), Value \\( V \\)) and a scattering operation on the output \\( \\tilde{O}_i \\).\n",
            "- **Optimization**: This transformation allows for the reuse of existing optimizations designed for vanilla attention mechanisms, such as flash attention.\n",
            "- **Efficiency**: Dilated attention offers a significant reduction in computational cost, achieving a decrease by a factor of \\( N r^2 \\) compared to traditional vanilla attention.\n",
            "\n",
            "This summary encapsulates the core concepts and benefits of dilated attention, making it suitable for retrieval purposes.\n",
            "\n",
            "---------\n",
            "# Summary of Figure 5: Runtime Comparison of Attention Mechanisms\n",
            "\n",
            "Figure 5 illustrates the runtime performance of two attention mechanisms: **vanilla attention** and **dilated attention**, both implemented using the **FlashAttention Kernel** to enhance memory efficiency and processing speed. \n",
            "\n",
            "## Key Findings:\n",
            "- **Dilated Attention**:\n",
            "  - Exhibits almost constant latency when scaling up sequence lengths.\n",
            "  - Capable of handling sequence lengths up to **1 billion tokens** by partitioning the sequence dimension and utilizing distributed systems.\n",
            "  - Demonstrates a significant advantage due to its **linear complexity** and the ability to implement distributed algorithms.\n",
            "\n",
            "- **Vanilla Attention**:\n",
            "  - Experiences a **quadratic dependency** on sequence length, leading to a dramatic increase in latency as the sequence length grows.\n",
            "  - Lacks a distributed algorithm, which limits its scalability and performance with longer sequences.\n",
            "\n",
            "## Conclusion:\n",
            "The results highlight the superior efficiency of dilated attention over vanilla attention, particularly in handling large-scale sequences, confirming the benefits of linear complexity and distributed processing in the LONGNET architecture.\n",
            "\n",
            "---------\n",
            "# Summary of Dilated Attention with Multiple Heads\n",
            "\n",
            "**Key Concepts:**\n",
            "- **Set of Weights (w):** The weights are represented as a set \\( w = \\{w_0, w_1, w_2, \\ldots, w_N\\} \\).\n",
            "- **Ordering of Weights:** The weights are ordered such that \\( w_i < w_{i+1} < N \\) for all \\( i \\).\n",
            "- **Dilated Attention Mechanism:** The attention patterns vary across different heads by successively shifting the positions.\n",
            "\n",
            "**Figure Reference:**\n",
            "- **Figure 3:** Illustrates the concept of dilated attention with multiple heads, highlighting the differences in attention patterns among the heads.\n",
            "\n",
            "This summary encapsulates the essential elements of the table or text, focusing on the structure and function of dilated attention in a multi-head context, which is crucial for understanding the underlying mechanisms in attention models.\n",
            "\n",
            "---------\n",
            "The image depicts a graph comparing the runtime performance of two attention mechanisms—dilated attention with FlashAttention and vanilla attention with FlashAttention—across varying sequence lengths. \n",
            "\n",
            "### Graph Details:\n",
            "\n",
            "- **X-Axis**: This axis represents the sequence length, marked in ascending order from 8K to 1B (1 billion), with tick marks at intervals of 16K, 32K, 64K, 128K, 512K, 2M, 8M, 32M, and 128M.\n",
            "\n",
            "- **Y-Axis**: This axis indicates the runtime in milliseconds (ms), spanning from just below 1000 ms to slightly above 5000 ms.\n",
            "\n",
            "### Data Representation:\n",
            "\n",
            "- **Lines**: \n",
            "  - **Dilated Attention (Blue Line)**: The graph shows that this method maintains a relatively low and stable runtime, hovering around 1000 ms up to a sequence length of around 64K. After this point, the runtime remains constant, reflecting excellent scalability.\n",
            "  - **Vanilla Attention (Orange Line)**: In contrast, the vanilla attention exhibits a sharp increase in runtime at about 64K sequence length, rapidly escalating to over 5000 ms at 128K, indicating a significant drop in efficiency as sequence lengths increase.\n",
            "\n",
            "### Legend:\n",
            "- A legend identifies the blue line as \"Dilated attention w/ FlashAttention\" and the orange line as \"Vanilla attention w/ FlashAttention.\"\n",
            "\n",
            "### Overall Interpretation:\n",
            "The graph effectively illustrates the advantages of dilated attention in handling longer sequences compared to vanilla attention, highlighting its superior performance in terms of runtime efficiency. This information is critical for researchers and practitioners looking to optimize transformer models, especially in scenarios involving large datasets or lengthy input sequences.\n",
            "\n",
            "---------\n",
            "# Summary of Runtime Comparison of Attention Mechanisms\n",
            "\n",
            "**Figure 5 Overview:**\n",
            "- **Focus:** Comparison of runtime performance between dilated attention and vanilla attention mechanisms.\n",
            "- **Enhancement:** Both attention types utilize FlashAttention as referenced in DFE 22.\n",
            "\n",
            "**Key Points:**\n",
            "- **Dilated Attention:** A variant of attention that potentially improves efficiency.\n",
            "- **Vanilla Attention:** The standard attention mechanism for baseline comparison.\n",
            "- **Performance Metrics:** The runtime of each mechanism is measured and compared.\n",
            "\n",
            "This summary encapsulates the essential details of the runtime analysis presented in Figure 5, highlighting the focus on dilated versus vanilla attention and the use of FlashAttention.\n",
            "\n",
            "---------\n",
            "# Summary of Dilated Attention Complexity\n",
            "\n",
            "- **Constants and Variables**:\n",
            "  - **w0**: Predefined constant.\n",
            "  - **α**: Common ratio for geometric sequences w and r.\n",
            "\n",
            "- **Computational Complexity**:\n",
            "  - The complexity of dilated attention is approximately **O(N d)**, where:\n",
            "    - **N**: Represents the number of elements.\n",
            "    - **d**: Represents the dimensionality.\n",
            "\n",
            "This summary encapsulates the key elements regarding the constants, variables, and computational complexity associated with dilated attention, making it easy to retrieve the relevant information.\n",
            "\n",
            "---------\n",
            "# Summary of Figure 2: Building Blocks of Dilated Attention in LONGNET\n",
            "\n",
            "**Description**: Figure 2 illustrates the architecture of dilated attention utilized in LONGNET. This framework is designed to effectively model both short-range and long-range dependencies within sequences.\n",
            "\n",
            "**Key Features**:\n",
            "- **Attention Patterns**: The model incorporates a variety of attention patterns that can be adjusted based on the sequence length.\n",
            "- **Scalability**: The number of attention patterns is extendable, allowing for flexibility in handling different sequence lengths.\n",
            "\n",
            "**Purpose**: The dilated attention mechanism aims to enhance the model's ability to capture dependencies across varying distances in the input data.\n",
            "\n",
            "**Application**: This approach is particularly beneficial for tasks requiring comprehensive context understanding over long sequences.\n",
            "\n",
            "---\n",
            "\n",
            "This summary is optimized for retrieval, highlighting the main components and functionalities of the dilated attention mechanism in LONGNET.\n",
            "\n",
            "---------\n",
            "# Summary of Attention Mechanism Implementation\n",
            "\n",
            "In this implementation of attention mechanisms, two key strategies are employed to enhance efficiency while maintaining the ability to capture both long-range and short-range information:\n",
            "\n",
            "1. **Segment Size (w)**: This parameter balances the globality of attention with computational efficiency. By adjusting the segment size, the model can optimize performance based on the specific requirements of the task.\n",
            "\n",
            "2. **Dilation Size (r)**: This parameter reduces computation costs by approximating the attention matrix. Dilation allows the model to focus on relevant information without processing the entire matrix, thus improving efficiency.\n",
            "\n",
            "3. **Mixture of Dilated Attentions**: The approach utilizes a combination of dilated attentions characterized by varying segment sizes and dilation rates, denoted as {ri, wi}. This mixture enables the model to effectively capture a wide range of information across different scales.\n",
            "\n",
            "This summary encapsulates the core concepts of the attention mechanism's implementation, focusing on the trade-offs between efficiency and information capture.\n",
            "\n",
            "---------\n",
            "# Summary of \"Attention is All You Need\"\n",
            "\n",
            "**Citation**:  \n",
            "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017). Attention is all you need. In *NeurIPS 2017* (pp. 5998–6008).\n",
            "\n",
            "**Key Points**:\n",
            "- **Authors**: The paper is authored by a team of researchers including Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin.\n",
            "- **Publication**: Presented at the NeurIPS conference in 2017.\n",
            "- **Pages**: The work spans pages 5998 to 6008.\n",
            "- **Main Contribution**: Introduces the Transformer model, which relies entirely on self-attention mechanisms, eliminating the need for recurrent or convolutional layers.\n",
            "- **Significance**: This architecture has significantly influenced the field of natural language processing (NLP) and has led to advancements in various applications, including machine translation and text generation.\n",
            "\n",
            "**Keywords**: Attention mechanism, Transformer model, natural language processing, NeurIPS 2017, self-attention, machine translation.\n",
            "\n",
            "---------\n"
          ]
        }
      ],
      "source": [
        "retriever = vectorstore.as_retriever()\n",
        "docs = retriever.invoke(\"What is Dialated attention?\", k=10)\n",
        "for doc in docs:\n",
        "    if is_base64(doc.page_content):\n",
        "        plt_img_base64(doc.page_content)\n",
        "        print('---------')\n",
        "    else:\n",
        "        print(doc.page_content)\n",
        "        print('\\n---------')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "oo",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
